# Fine-Tuning Small Language Models (SLMs) with Large Language Models (LLMs)

This repository demonstrates how to fine-tune a Small Language Model (SLM) using synthetic data generated by a Large Language Model (LLM). The tutorial covers data generation, fine-tuning, evaluation, and inference, providing a complete workflow for creating efficient and task-specific AI models.

## Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [Setup](#setup)
- [Usage](#usage)
  - [Step 1: Generate Synthetic Data](#step-1-generate-synthetic-data)
  - [Step 2: Fine-Tune the SLM](#step-2-fine-tune-the-slm)
  - [Step 3: Evaluate the Model](#step-3-evaluate-the-model)
  - [Step 4: Run Inference](#step-4-run-inference)
- [Repository Structure](#repository-structure)
- [Dependencies](#dependencies)
- [License](#license)

---

## Overview
Fine-tuning Small Language Models (SLMs) with Large Language Models (LLMs) is a powerful approach to creating efficient and specialized AI models. This repository provides a hands-on tutorial to:
1. Generate synthetic training data using an LLM (e.g., GPT-4).
2. Fine-tune a smaller, more efficient model (e.g., DistilBERT) on the generated data.
3. Evaluate the fine-tuned model's performance.
4. Deploy the model for inference on new data.

This approach is ideal for resource-constrained environments where deploying large models is impractical.

---

## Key Features
- **Data Augmentation**: Use an LLM to generate high-quality synthetic data for fine-tuning.
- **Efficient Fine-Tuning**: Fine-tune a lightweight SLM (e.g., DistilBERT) for specific tasks.
- **End-to-End Workflow**: From data generation to model deployment, this repository provides a complete pipeline.
- **Customizable**: Easily adapt the code for other tasks like text classification, named entity recognition, or question answering.

---

## Setup
1. Clone the repository:
   ```bash
   git clone https://github.com/a24ibrah/SLMWzLLM.git
   cd fine-tuning-slm-with-llm
   ```
2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Set your OpenAI API key in the `generate_data.py` script:
   ```python
   openai.api_key = "your_openai_api_key"
   ```

---

## Usage

### Step 1: Generate Synthetic Data
Use an LLM (e.g., GPT-4) to generate a synthetic dataset for fine-tuning. Run the following script:
```bash
python generate_data.py
```
This will create a `sentiment_dataset.txt` file containing labeled examples for sentiment analysis.

### Step 2: Fine-Tune the SLM
Fine-tune a Small Language Model (e.g., DistilBERT) on the generated dataset. Run the following script:
```bash
python fine_tune_slm.py
```
This will:
- Preprocess the dataset.
- Fine-tune the model using Hugging Face's Trainer API.
- Save the fine-tuned model to the `fine-tuned-sentiment-model` directory.

### Step 3: Evaluate the Model
Evaluate the fine-tuned model's performance on the validation set. Run the following script:
```bash
python evaluate_model.py
```
This will print evaluation metrics such as accuracy and loss.

### Step 4: Run Inference
Use the fine-tuned model to predict sentiment on new text. Run the following script:
```bash
python inference.py
```
Example output:
```python
[{'label': 'positive', 'score': 0.99}]
```

---

## Repository Structure
```
fine-tuning-slm-with-llm/
│
├── README.md                       # Project overview and instructions
├── requirements.txt                # Python dependencies
├── generate_data.py                # Script to generate synthetic data using an LLM
├── fine_tune_slm.py                # Script to fine-tune the SLM
├── evaluate_model.py               # Script to evaluate the fine-tuned model
├── inference.py                    # Script to run inference with the fine-tuned model
├── sentiment_dataset.txt           # Generated dataset (output of generate_data.py)
├── results/                        # Directory for training outputs
└── fine-tuned-sentiment-model/     # Directory for the fine-tuned model (output of fine_tune_slm.py)
```

---

## Dependencies
- Python 3.8+
- Libraries:
  - `transformers` (Hugging Face)
  - `datasets` (Hugging Face)
  - `torch` (PyTorch)
  - `openai` (OpenAI API)

Install all dependencies using:
```bash
pip install -r requirements.txt
```

---

## License
This project is licensed under the MIT License. See the LICENSE file for details.

---

## Contributing
Contributions are welcome! If you have suggestions or improvements, please open an issue or submit a pull request.

---

## Acknowledgments
- Hugging Face for the `transformers` and `datasets` libraries.
- OpenAI for providing access to powerful LLMs like GPT-4.
